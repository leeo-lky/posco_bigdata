{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/home/piai/AI_Pytorch/posco-aibd/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "# ============================== Do not edit this shell ==========================================\n",
    "# Dataset Definition\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, root, train, transform=None):\n",
    "        self.train = train\n",
    "        if self.train:\n",
    "            root = root + 'train.csv'\n",
    "        else:\n",
    "            root = root + 'test.csv'\n",
    "        self.csv = pd.read_csv(root, header=None)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            label = torch.tensor(self.csv.iloc[index,0], dtype=torch.long)\n",
    "            img = np.array(self.csv.iloc[index,1:]/255).reshape(28, 28)\n",
    "            img = Image.fromarray(img)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, label\n",
    "        else:\n",
    "            img = np.array(self.csv.loc[index]/255).reshape(28, 28)\n",
    "            img = Image.fromarray(img)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img\n",
    "# ============================== Do not edit this shell ==========================================\n",
    "# Library Importation\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from IPython.display import clear_output\n",
    "from skimage.transform import resize as OVResize\n",
    "from torch.cuda import memory_allocated, empty_cache\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchsummary import summary as Summary\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomHorizontalFlip, \\\n",
    "                                   ToPILImage, Resize, Grayscale\n",
    "# Hyper Parameter\n",
    "## Data Loader\n",
    "batch_size = 32\n",
    "# ## Model\n",
    "# hidden_layer = 220\n",
    "## Learning\n",
    "logging_dispfig = True\n",
    "maximum_epoch = 20\n",
    "learning_rate = 0.004\n",
    "# Device Preparation\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'{\"CPU\" if device == \"cpu\" else \"GPU\"} will be used in training/validation.')\n",
    "input_transform = transforms.Compose([\n",
    "    Resize(48),\n",
    "    #RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "])\n",
    "# Prepare dataset\n",
    "root = './'\n",
    "train_data = CharDataset(root, train=True, transform=input_transform)\n",
    "# train_data, valid_data = random_split(train_data, [round(len(train_data)*0.9), round(len(train_data)*0.1)])\n",
    "tt, valid_data = random_split(train_data, [round(len(train_data)*0.9), round(len(train_data)*0.1)])\n",
    "test_data = CharDataset(root, train=False, transform=input_transform)\n",
    "# Check the data\n",
    "print('===================== Check the data =========================\\n')\n",
    "print(f'Train dataset length = {len(train_data)}')\n",
    "print(f'Valid dataset length = {len(valid_data)}')\n",
    "print(f'Test dataset length = {len(test_data)}\\n')\n",
    "train_0_x, train_0_y = train_data[0]\n",
    "print(f'Content of Y (Label, type={type(train_0_y)}) = {train_0_y}')\n",
    "print(f'Shape of X (Data, type={type(train_0_x)}) = {train_0_x.shape}')\n",
    "plt.figure(1)\n",
    "plt.imshow(train_0_x.squeeze())\n",
    "plt.title(f'train_0_x ({train_0_x.squeeze().shape})')\n",
    "plt.show()\n",
    "# Create data loader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                          drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=len(valid_data), pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=len(test_data), pin_memory=True)\n",
    "# Examine the data loader\n",
    "print('================== Check the data loader ======================\\n')\n",
    "train_enumerator = enumerate(train_loader)\n",
    "ex_batch_idx, (ex_data, ex_label) = next(train_enumerator)\n",
    "print(f'Idx: {ex_batch_idx} / X.shape = {ex_data.shape} / Y.shape = {ex_label.shape}\\n')\n",
    "print(f'Y[0:{batch_size}] = {ex_label}')\n",
    "preview_index = 0\n",
    "len(train_data)\n",
    "def init_model():\n",
    "    global net, loss_fn, optim\n",
    "    net = MyLittleCNN().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optim = Adam(net.parameters(), lr=learning_rate)\n",
    "class MyLittleCNN(nn.Module):\n",
    "#  \"\"\"My Little Convolutional Neural Network for Active CAM, based on VGG11\"\"\"\n",
    "    def __init__(self):\n",
    "        super(MyLittleCNN, self).__init__()\n",
    "        self.convolution_part = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.channelavg_part = nn.AvgPool2d(6)\n",
    "        self.classifier_part = nn.Linear(64, 10, bias=False)\n",
    "    def forward(self, data):\n",
    "        conv_out = self.convolution_part(data)\n",
    "        avg_out = self.channelavg_part(conv_out)\n",
    "        avg_out_flatten = avg_out.reshape(avg_out.size(0), -1)\n",
    "        classifier_out = self.classifier_part(avg_out_flatten)\n",
    "        return classifier_out, conv_out\n",
    "Summary(MyLittleCNN().to(device), (1, 48, 48))\n",
    "# Memory cleaner to prevent CUDA out of memory error\n",
    "def clear_memory():\n",
    "    if device != 'cpu':\n",
    "        empty_cache()\n",
    "    gc.collect()\n",
    "def init_epoch():\n",
    "  global epoch_cnt\n",
    "  epoch_cnt = 0\n",
    "def epoch(data_loader):\n",
    "  # One epoch : gets data_loader as input and returns loss / accuracy, and\n",
    "  #             last prediction value / its label(truth) value for future use\n",
    "  global epoch_cnt\n",
    "  iter_loss, iter_acc = [], []\n",
    "  last_grad_performed = False\n",
    "  # Mini-batch iterations\n",
    "  for _data, _label in data_loader:\n",
    "    data, label = _data.to(device), _label.to(device)\n",
    "    # 1. Feed-forward\n",
    "    onehot_out, _ = net(data)\n",
    "    # 2. Calculate accuracy\n",
    "    _, out = torch.max(onehot_out, 1)\n",
    "    acc_partial = (out == label).float().sum()\n",
    "    acc_partial = acc_partial / len(label)\n",
    "    iter_acc.append(acc_partial.item())\n",
    "    # 3. Calculate loss\n",
    "    loss = loss_fn(onehot_out, label)\n",
    "    iter_loss.append(loss.item())\n",
    "    # 4. Backward propagation if not in `torch.no_grad()`\n",
    "    if onehot_out.requires_grad:\n",
    "      optim.zero_grad()\n",
    "      loss.backward()\n",
    "      optim.step()\n",
    "      last_grad_performed = True\n",
    "  # Up epoch count if backward propagation is done\n",
    "  if last_grad_performed:\n",
    "    epoch_cnt += 1\n",
    "  # Clear memory to prevent CUDA memory error\n",
    "  clear_memory()\n",
    "  return np.average(iter_loss), np.average(iter_acc)\n",
    "def epoch_not_finished():\n",
    "  # For now, let's repeat training fixed times, e.g. 25 times.\n",
    "  # We will learn how to determine training stop or continue later.\n",
    "  return epoch_cnt < maximum_epoch\n",
    "# Logging\n",
    "def init_log():\n",
    "    global log_stack, iter_log, tloss_log, tacc_log, vloss_log, vacc_log, time_log\n",
    "    iter_log, tloss_log, tacc_log, vloss_log, vacc_log = [], [], [], [], []\n",
    "    time_log, log_stack = [], []\n",
    "def record_train_log(_tloss, _tacc, _time):\n",
    "    # Push time, training loss, training accuracy, and epoch count into lists\n",
    "    time_log.append(_time)\n",
    "    tloss_log.append(_tloss)\n",
    "    tacc_log.append(_tacc)\n",
    "    iter_log.append(epoch_cnt)\n",
    "def record_valid_log(_vloss, _vacc):\n",
    "    # Push validation loss and validation accuracy into each list\n",
    "    vloss_log.append(_vloss)\n",
    "    vacc_log.append(_vacc)\n",
    "def last(log_list):\n",
    "    # Get the last member of list. If empty, return -1.\n",
    "    if len(log_list) > 0: return log_list[len(log_list) - 1]\n",
    "    else: return -1\n",
    "def print_log():\n",
    "    # Generate log string and put it into log stack\n",
    "    log_str = f'Iter: {last(iter_log):>4d} >> T_loss {last(tloss_log):<8.5f}   ' \\\n",
    "          + f'T_acc {last(tacc_log):<6.5f}   V_loss {last(vloss_log):<8.5f}   ' \\\n",
    "          + f'V_acc {last(vacc_log):<6.5f}   :시계_3시: {last(time_log):5.3f}s'\n",
    "    log_stack.append(log_str)\n",
    "  # Draw figure if want\n",
    "    if logging_dispfig:\n",
    "        hist_fig, loss_axis = plt.subplots(figsize=(10, 3), dpi=99)\n",
    "        hist_fig.patch.set_facecolor('white')\n",
    "        # Draw loss lines\n",
    "        loss_t_line = plt.plot(iter_log, tloss_log, label='Train Loss', color='#FF9999', marker='o')\n",
    "        loss_v_line = plt.plot(iter_log, vloss_log, label='Valid Loss', color='#99B0FF', marker='s')\n",
    "        loss_axis.set_xlabel('epoch')\n",
    "        loss_axis.set_ylabel('loss')\n",
    "        # Draw accuracy lines\n",
    "        acc_axis = loss_axis.twinx()\n",
    "        acc_t_line = acc_axis.plot(iter_log, tacc_log, label='Train Acc.', color='#FF0000', marker='+')\n",
    "        acc_v_line = acc_axis.plot(iter_log, vacc_log, label='Valid Acc.', color='#003AFF', marker='x')\n",
    "        acc_axis.set_ylabel('accuracy')\n",
    "        # Append annotations\n",
    "        hist_lines = loss_t_line + loss_v_line + acc_t_line + acc_v_line\n",
    "        loss_axis.legend(hist_lines, [l.get_label() for l in hist_lines])\n",
    "        loss_axis.grid()\n",
    "        plt.title(f'Learning history until epoch {last(iter_log)}')\n",
    "        plt.draw()\n",
    "  # Print log\n",
    "    clear_output(wait=True)\n",
    "    if logging_dispfig: plt.show()\n",
    "    for idx in reversed(range(len(log_stack))):\n",
    "        print(log_stack[idx])\n",
    "# hidden_layer = 220\n",
    "# Training Initialization\n",
    "maximum_epoch = 35\n",
    "init_model()\n",
    "init_epoch()\n",
    "init_log()\n",
    "# Training Iteration\n",
    "while epoch_not_finished():\n",
    "    start_time = time.time()\n",
    "    tloss, tacc = epoch(train_loader)\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    record_train_log(tloss, tacc, time_taken)\n",
    "    with torch.no_grad():\n",
    "        vloss, vacc = epoch(valid_loader)\n",
    "        record_valid_log(vloss, vacc)\n",
    "    print_log()\n",
    "print('\\n Training completed!')\n",
    " # Save prediction vector to CSV file\n",
    "# Before run this code, here put your save path (only for local server not kaggle kernel)\n",
    "save_root = './test_pred.csv'   # <--- only edit this path\n",
    "# After run this code, you must check that the shape of 'out' variable is 3745. (out.shape == 3745)\n",
    "# ============================== Do not edit under this line ==========================================\n",
    "for _data in test_loader:\n",
    "    data = _data.to(device)\n",
    "    # 1. Feed-forward\n",
    "    onehot_out, _ = net(data)\n",
    "    _, out = torch.max(onehot_out, 1)\n",
    "print(out.shape)\n",
    "import csv\n",
    "# 덮어쓰기 방지를 위해 이미 파일이 존재하면 삭제\n",
    "if os.path.isfile(save_root):\n",
    "    os.remove(save_root)\n",
    "# 첫 행에 'id' 'lable' 그 다음 행부터 idx와 label 넣어서 csv 저장\n",
    "for idx, pred in enumerate(list(out.cpu())):\n",
    "    with open(save_root, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if idx == 0:\n",
    "            writer.writerow(['id', 'label'])\n",
    "        pred = np.concatenate(([idx], [pred]))\n",
    "        writer.writerow(pred)\n",
    "# ============================== Do not edit over this line =========================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
